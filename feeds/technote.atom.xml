<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>LeafHub</title><link href="http://leafwind.tw/" rel="alternate"></link><link href="http://leafwind.tw/feeds/technote.atom.xml" rel="self"></link><id>http://leafwind.tw/</id><updated>2016-02-19T00:00:00+08:00</updated><entry><title>解決 python 在讀取 .json 檔案時佔用記憶體過多的問題</title><link href="http://leafwind.tw/20160219-release-python-memory-by-sub-process.html" rel="alternate"></link><updated>2016-02-19T00:00:00+08:00</updated><author><name>leafwind</name></author><id>tag:leafwind.tw,2016-02-19:20160219-release-python-memory-by-sub-process.html</id><summary type="html">&lt;h2&gt;故事是這樣的&lt;/h2&gt;
&lt;p&gt;某天我發現原本執行都沒有問題的程式突然掛了&lt;/p&gt;
&lt;p&gt;仔細一查發現是記憶體使用過多，因為有別人的程式需要資源，跑到一半就被 OS 給砍了&lt;/p&gt;
&lt;p&gt;（題外話，砍的優先順序包括你的程式跑了多久、吃多少記憶體等等一堆參數）&lt;/p&gt;
&lt;p&gt;而我心中以為，某些用不到的 object reference 在 function return 之後就會釋放&lt;/p&gt;
&lt;p&gt;顯然並沒有&lt;/p&gt;
&lt;p&gt;於是先找了一些方法像是 &lt;code&gt;del&lt;/code&gt; 或 &lt;code&gt;gc.collect()&lt;/code&gt; 試圖手動釋放記憶體&lt;/p&gt;
&lt;p&gt;但實驗證實也完全沒有用&lt;/p&gt;
&lt;p&gt;最後找到一篇文章 (1)&lt;/p&gt;
&lt;h4&gt;python 要釋放指定的 object 所佔用的 memory 原來要用到 sub-process&lt;/h4&gt;
&lt;p&gt;著實讓我驚呆了，特此紀念解決的過程：&lt;/p&gt;
&lt;h2&gt;問題開始&lt;/h2&gt;
&lt;p&gt;給定一個 &lt;code&gt;.json&lt;/code&gt; file，經過 &lt;code&gt;json.load()&lt;/code&gt; 或 &lt;code&gt;json.loads()&lt;/code&gt; 讀取為 object 之後  &lt;/p&gt;
&lt;p&gt;佔用的 memory 可能會是原本 text file 的 8~10 倍（ex: 400M file =&amp;gt; 5G memory，取決於 object content）  &lt;/p&gt;
&lt;p&gt;若這個 json file 本來就很大，讀取後系統資源可能就不夠，甚至有機會被 kernel kill  &lt;/p&gt;
&lt;p&gt;就算最後的 output 很小，使用 &lt;code&gt;del&lt;/code&gt; 或 &lt;code&gt;gc.collect()&lt;/code&gt; 仍然無法在系統端看到記憶體被釋放&lt;/p&gt;
&lt;p&gt;這個問題其實由兩個部分組成&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;沒有一個直接的方式釋放 process 在讀檔過程中所佔用的記憶體&lt;/li&gt;
&lt;li&gt;python 讀取 json 檔案非常佔用記憶體&lt;/li&gt;
&lt;/ol&gt;
&lt;h2&gt;Solution1. sub-process (multiprocessing)&lt;/h2&gt;
&lt;p&gt;使用 pyhton 內建的 &lt;code&gt;multiprocessing&lt;/code&gt; module&lt;/p&gt;
&lt;p&gt;可以間接做到「釋放用不到的記憶體」&lt;/p&gt;
&lt;p&gt;我不知道 &lt;code&gt;multiprocessing&lt;/code&gt; 這個 module 當初開發的目的有沒有包含記憶體釋放&lt;/p&gt;
&lt;p&gt;但它確實可以做到 &lt;code&gt;del&lt;/code&gt; 及 &lt;code&gt;gc.collect()&lt;/code&gt; 做不到的事情...&lt;/p&gt;
&lt;h3&gt;pros:&lt;/h3&gt;
&lt;p&gt;若程式處理完 &lt;code&gt;.json&lt;/code&gt; 之後只會剩下少部分資料&lt;/p&gt;
&lt;p&gt;我們可以把「讀檔-&amp;gt;處理」這一段 code 寫成 subprocess&lt;/p&gt;
&lt;p&gt;這樣只要 subprocess 執行完並 exit 後&lt;/p&gt;
&lt;p&gt;所有暫時佔用的記憶體在 system 端就會被確實釋放&lt;/p&gt;
&lt;h3&gt;cons:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;inter-process 之間的 object 傳遞似乎有某些限制：&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;object size 大到某個程度（ex. dict key size &amp;gt; 40k）之後就會卡死&lt;/p&gt;
&lt;p&gt;（這個詞不精確，其實是跑很久沒回應，但不確定最後會不會停）&lt;/p&gt;
&lt;p&gt;不過幾百個 key 的 dict 還是 OK 的&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;當 &lt;code&gt;.json&lt;/code&gt; 檔案大到某個程度，一次讀取完所需記憶體比可用記憶體還大的話，此法就沒用了，只能求助法二。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Solution2. ijson (streaming)&lt;/h2&gt;
&lt;h3&gt;pros:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;不管多大的 json file 都能處理（？）&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;cons:&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;需要更動程式架構&lt;/li&gt;
&lt;li&gt;ijson 雖然非常省記憶體，但讀取速度可能也較慢&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;（尚未做 benchmark，也無優化過，純粹體感）&lt;/p&gt;
&lt;p&gt;[1.] [Huge memory usage of Python's json module?] (http://stackoverflow.com/questions/11057712/huge-memory-usage-of-pythons-json-module)&lt;/p&gt;</summary><category term="python"></category><category term="memory"></category><category term="leak"></category><category term="json"></category></entry></feed>